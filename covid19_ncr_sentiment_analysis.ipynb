{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:41:50.598331Z",
     "start_time": "2021-01-18T14:41:14.126560Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords.words('english')\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:42:14.578723Z",
     "start_time": "2021-01-18T14:42:13.218695Z"
    }
   },
   "outputs": [],
   "source": [
    "#!dir ..\\..\\1. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:43:29.249501Z",
     "start_time": "2021-01-18T14:43:29.244515Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_tweets = pd.read_csv(r\"../../1. Datasets/NLP Datasets/2. Labelled Tweets/ncr.csv\")\n",
    "df_tweets = pd.read_csv(\"ncr.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:47:40.860660Z",
     "start_time": "2021-01-18T14:47:40.850687Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text, tagalog_letters):\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "    text = tagalog_letters(text)\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = sub(r\"[^A-Za-z0-9^,!?.\\/'+]\", \" \", text)\n",
    "    text = sub(r\"http\\S.*$\",\" \", text) # removing twitter urls\n",
    "    text = sub(r\"\\+\", \" plus \", text)\n",
    "    text = sub(r\",\", \" \", text)\n",
    "    text = sub(r\"\\.\", \" \", text)\n",
    "    text = sub(r\"!\", \" ! \", text)\n",
    "    text = sub(r\"\\?\", \" ? \", text)\n",
    "    text = sub(r\"'\", \"\", text)\n",
    "    text = sub(r\":\", \" : \", text)\n",
    "    text = sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "        \n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        \n",
    "    text = [word for word in text.split(' ') if word not in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:47:46.493406Z",
     "start_time": "2021-01-18T14:47:46.211162Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.tweets = df_tweets.tweets.apply(lambda x: text_to_word_list(x, unidecode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      mayor jerry trenas representing iloilo city go...\n",
       "1      online muna tayo mga kap para sa bayan sa mga ...\n",
       "2      best feeling 8 hours shift covid unit removing...\n",
       "3      dont forget covid 19 safety protocols us still...\n",
       "4      taena nitong covid parang produkto daming vari...\n",
       "                             ...                        \n",
       "177    nakaka sad doctors nurses died frontliners cov...\n",
       "178    dati everyday kung mag update ang doh covid re...\n",
       "179    78 efficacy 100 prevention severe cases covid ...\n",
       "180     kitsunemaisonn chefadobo hahahahaahaa love ko...\n",
       "181    eto kabado na naman kami sa office 3 na pala c...\n",
       "Name: tweets, Length: 182, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Usually splitting is done before vectorization (i.e. the vectorizer shouldn't be fitted with the test set of tweets). However, since we have a small dataset, it is more likely for words in test set to not be available in the train set (which will cause inaccuracies in the model). So in this case, let's just vectorize the full dataset before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:49:13.629390Z",
     "start_time": "2021-01-18T14:49:12.298642Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:50:59.484905Z",
     "start_time": "2021-01-18T14:50:59.468948Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(norm=None)\n",
    "tfidf.fit(df_tweets.tweets)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "X = pd.DataFrame(tfidf.transform(df_tweets.tweets).todense(), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:53:45.915412Z",
     "start_time": "2021-01-18T14:53:45.912420Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df_tweets.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:19.331779Z",
     "start_time": "2021-01-18T14:54:19.305849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (145, 1175)\n",
      "X_val shape:  (37, 1175)\n",
      "y_train shape:  (145,)\n",
      "y_val shape:  (37,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "# X_train_text = X_train['tweets']\n",
    "# X_val_text = X_val['tweets']\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_val shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:34.986268Z",
     "start_time": "2021-01-18T14:54:34.652923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Classifier Modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "stratk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:38.641243Z",
     "start_time": "2021-01-18T14:54:38.253975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6        0.46666667 0.53333333 0.46666667 0.53333333 0.57142857\n",
      " 0.64285714 0.35714286 0.35714286 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "kfold_score = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(kfold_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:45.580750Z",
     "start_time": "2021-01-18T14:54:45.575763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.29"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kfold logistic regression score\n",
    "round(np.mean(kfold_score)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grids = {'learning_rate': [.2, 0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [3, 4, 6, 10, 14],\n",
    "              'min_samples_leaf': [2, 3, 4],\n",
    "              'max_features': [.5,.3, .2] \n",
    "}   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This code is written a few years ago in my ML class (a bit outdated). Will try to find the updated code and update. This does manual gridsearch of hyperparameters. Actual implementation of GridSearchCV will be faster and better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T15:17:29.385385Z",
     "start_time": "2021-01-18T15:17:29.321544Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import pandas as pd\n",
    "import pylab as plot\n",
    "import lmfit as lf\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pandas import Series\n",
    "from math import sqrt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC, LinearSVC, LinearSVR\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "\n",
    "import xgboost as XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "\n",
    "import numpy\n",
    "\n",
    "def classification(X, y, scaler=None):\n",
    "    kneighbors = range(1, 20)\n",
    "    smoothing = np.logspace(1, 15, num=10)\n",
    "    a = np.logspace(-5, 5, num=10)\n",
    "    gamma_list = np.logspace(-5, 5, num=10)\n",
    "    depth_settings = range(1, 15)\n",
    "    tolerance = np.logspace(-5, 5, num=10)\n",
    "    \n",
    "    knn_c = KNeighborsClassifier(n_jobs=-1)\n",
    "    log_res_l1 = LogisticRegression(penalty='l1', max_iter=1000,\n",
    "                             solver='liblinear', n_jobs=-1)\n",
    "    log_res_l2 = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "    lin_svc_l1 = LinearSVC(penalty='l1', dual=False, max_iter=10000)\n",
    "    lin_svc_l2 = LinearSVC(penalty='l2', max_iter=10000)\n",
    "    # cl7 = SVC(kernel='poly', degree=3)\n",
    "    nsvm_rbf = SVC(kernel='rbf')\n",
    "    d_tree = DecisionTreeClassifier(random_state=0, max_depth=depth_settings)\n",
    "    r_forest = RandomForestClassifier(max_depth=depth_settings, random_state=0)\n",
    "    xg_boost = XGBClassifier.XGBClassifier(random_state=0, max_depth=depth_settings, learning_rate=0.01)\n",
    "#     mult_nb = MultinomialNB(alpha=a)\n",
    "#     n_bayes = GaussianNB()\n",
    "    \n",
    "\n",
    "    est = [('KNN', knn_c, {'n_neighbors':kneighbors}),\n",
    "           ('Logistic Regression (L1)', log_res_l1, {'C':a}),\n",
    "           ('Logistic Regression (L2)', log_res_l2, {'C':a}),\n",
    "           ('Linear SVM (L1)', lin_svc_l1, {'C':a}),\n",
    "           ('Linear SVM (L2)', lin_svc_l2, {'C':a}),\n",
    "           ('NonLinear SVM (RBF)', nsvm_rbf, {'C':a, 'gamma':gamma_list}),\n",
    "           ('Decision Tree',d_tree,{'max_depth':depth_settings}),\n",
    "           ('Random Forest',r_forest,{'max_depth':depth_settings}),\n",
    "           ('XGBoost',xg_boost,{'max_depth':depth_settings})]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "    \n",
    "    if scaler == 'ss':\n",
    "        scale = StandardScaler()\n",
    "        X_train = scale.fit_transform(X = X_train)\n",
    "        X_val = scale.transform(X = X_val)\n",
    "    elif scaler == 'mm':\n",
    "        scale = MinMaxScaler()\n",
    "        X_train = scale.fit_transform(X_train)\n",
    "        X_val = scale.transform(X_val)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    models = {}\n",
    "    for e in est:\n",
    "        print(f'Training {e[0]}')\n",
    "        start_time = time.time()\n",
    "        gs_cv = GridSearchCV(e[1], param_grid=e[2], n_jobs=-1)\n",
    "        gs_cv.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        models[e[0]] = gs_cv, elapsed_time\n",
    "        print(f'Training {e[0]} complete in {elapsed_time} seconds \\n')\n",
    "    \n",
    "    accuracy_summary ={}\n",
    "    for m in models:\n",
    "        test_acc = models[m][0].best_estimator_.score(X_val, y_val)\n",
    "        train_acc = models[m][0].best_estimator_.score(X_train, y_train)\n",
    "        best_param = models[m][0].best_params_\n",
    "        accuracy_summary[m] = test_acc, train_acc, best_param, models[m][1]\n",
    "    \n",
    "    target_count = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((target_count/target_count.sum())**2)\n",
    "        \n",
    "    results_summary = pd.DataFrame.from_dict(accuracy_summary, orient='index', columns=['Test Accuracy', 'Training Accuracy', 'Best Parameters', 'Run Time'])\n",
    "    results_summary['Test Accuracy / PCC'] = results_summary['Test Accuracy']/pcc\n",
    "    \n",
    "    #linear\n",
    "    summary={}\n",
    "    methods_l = ['Logistic Regression (L1)','Logistic Regression (L2)',\n",
    "              'Linear SVM (L1)','Linear SVM (L2)']\n",
    "\n",
    "    for m in methods_l:\n",
    "        pred = []\n",
    "        for i,j in list(zip(X.columns, (models[m][0].best_estimator_.coef_))):\n",
    "            pred.append([i,j[np.abs(j).argmax()]])\n",
    "            summary[m] = pred\n",
    "\n",
    "    #ensamble\n",
    "    methods_e = ['Decision Tree', 'Random Forest','XGBoost']\n",
    "    pred = []\n",
    "    for m in methods_e:\n",
    "        coefs = [(X.columns[(models[m][0].best_estimator_.feature_importances_).argsort()[-3:]\n",
    "                           ], models[m][0].best_estimator_.feature_importances_[\n",
    "            (models[m][0].best_estimator_.feature_importances_).argsort()[-3:]])]\n",
    "\n",
    "        coeff = list(zip(*coefs[0]))\n",
    "        pred.append(coeff)\n",
    "        summary[m] = pred[0]\n",
    "\n",
    "    #no coeff\n",
    "    methods_no = ['KNN','NonLinear SVM (RBF)']\n",
    "    for m in methods_no:\n",
    "        summary[m] = [[np.nan],[np.nan],[np.nan]]\n",
    "\n",
    "    df_summary_c1 = pd.DataFrame.from_dict(summary)\n",
    "        \n",
    "    return results_summary.join(df_summary_c1.T, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T15:18:52.979704Z",
     "start_time": "2021-01-18T15:17:30.027287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN\n",
      "Training KNN complete in 4.638301372528076 seconds \n",
      "\n",
      "Training Logistic Regression (L1)\n",
      "Training Logistic Regression (L1) complete in 0.8062481880187988 seconds \n",
      "\n",
      "Training Logistic Regression (L2)\n",
      "Training Logistic Regression (L2) complete in 2.1716747283935547 seconds \n",
      "\n",
      "Training Linear SVM (L1)\n",
      "Training Linear SVM (L1) complete in 3.2298083305358887 seconds \n",
      "\n",
      "Training Linear SVM (L2)\n",
      "Training Linear SVM (L2) complete in 0.868689775466919 seconds \n",
      "\n",
      "Training NonLinear SVM (RBF)\n",
      "Training NonLinear SVM (RBF) complete in 11.626434326171875 seconds \n",
      "\n",
      "Training Decision Tree\n",
      "Training Decision Tree complete in 1.0874981880187988 seconds \n",
      "\n",
      "Training Random Forest\n",
      "Training Random Forest complete in 7.377640962600708 seconds \n",
      "\n",
      "Training XGBoost\n",
      "Training XGBoost complete in 65.94825005531311 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performances = classification(X,y,scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Test Accuracy / PCC</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>4.638301</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L1)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>0.806248</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.3943897028169658]</td>\n",
       "      <td>[10, -0.710828787650146]</td>\n",
       "      <td>[100, -0.6794881387204452]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L2)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.021544346900318846}</td>\n",
       "      <td>2.171675</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.0898000780119623]</td>\n",
       "      <td>[10, 0.07249670803438732]</td>\n",
       "      <td>[100, 0.1105365630480242]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L1)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 3.593813663804626}</td>\n",
       "      <td>3.229808</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, 0.9335408437389131]</td>\n",
       "      <td>[10, -1.3446957977410057]</td>\n",
       "      <td>[100, 0.46191392052260766]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L2)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.0016681005372000592}</td>\n",
       "      <td>0.868690</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, -0.03815140571885928]</td>\n",
       "      <td>[10, -0.07445835664585077]</td>\n",
       "      <td>[100, -0.06556584699866412]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonLinear SVM (RBF)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>{'C': 599.4842503189421, 'gamma': 1e-05}</td>\n",
       "      <td>11.626434</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>1.087498</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>(lgus, 0.2303345402528252)</td>\n",
       "      <td>(por, 0.25671081977659505)</td>\n",
       "      <td>(covid, 0.5129546399705798)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.751724</td>\n",
       "      <td>{'max_depth': 14}</td>\n",
       "      <td>7.377641</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>(lgus, 0.2303345402528252)</td>\n",
       "      <td>(por, 0.25671081977659505)</td>\n",
       "      <td>(covid, 0.5129546399705798)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.737931</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>65.948250</td>\n",
       "      <td>1.773180</td>\n",
       "      <td>(lgus, 0.2303345402528252)</td>\n",
       "      <td>(por, 0.25671081977659505)</td>\n",
       "      <td>(covid, 0.5129546399705798)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Test Accuracy  Training Accuracy  \\\n",
       "KNN                            0.540541           0.724138   \n",
       "Logistic Regression (L1)       0.621622           0.862069   \n",
       "Logistic Regression (L2)       0.621622           0.972414   \n",
       "Linear SVM (L1)                0.648649           1.000000   \n",
       "Linear SVM (L2)                0.648649           0.972414   \n",
       "NonLinear SVM (RBF)            0.621622           0.979310   \n",
       "Decision Tree                  0.540541           0.613793   \n",
       "Random Forest                  0.540541           0.751724   \n",
       "XGBoost                        0.675676           0.737931   \n",
       "\n",
       "                                                   Best Parameters   Run Time  \\\n",
       "KNN                                            {'n_neighbors': 12}   4.638301   \n",
       "Logistic Regression (L1)                 {'C': 0.2782559402207126}   0.806248   \n",
       "Logistic Regression (L2)               {'C': 0.021544346900318846}   2.171675   \n",
       "Linear SVM (L1)                           {'C': 3.593813663804626}   3.229808   \n",
       "Linear SVM (L2)                       {'C': 0.0016681005372000592}   0.868690   \n",
       "NonLinear SVM (RBF)       {'C': 599.4842503189421, 'gamma': 1e-05}  11.626434   \n",
       "Decision Tree                                     {'max_depth': 2}   1.087498   \n",
       "Random Forest                                    {'max_depth': 14}   7.377641   \n",
       "XGBoost                                           {'max_depth': 5}  65.948250   \n",
       "\n",
       "                          Test Accuracy / PCC                            0  \\\n",
       "KNN                                  1.418544                        [nan]   \n",
       "Logistic Regression (L1)             1.631326    [000, 0.3943897028169658]   \n",
       "Logistic Regression (L2)             1.631326    [000, 0.0898000780119623]   \n",
       "Linear SVM (L1)                      1.702253    [000, 0.9335408437389131]   \n",
       "Linear SVM (L2)                      1.702253  [000, -0.03815140571885928]   \n",
       "NonLinear SVM (RBF)                  1.631326                        [nan]   \n",
       "Decision Tree                        1.418544   (lgus, 0.2303345402528252)   \n",
       "Random Forest                        1.418544   (lgus, 0.2303345402528252)   \n",
       "XGBoost                              1.773180   (lgus, 0.2303345402528252)   \n",
       "\n",
       "                                                   1  \\\n",
       "KNN                                            [nan]   \n",
       "Logistic Regression (L1)    [10, -0.710828787650146]   \n",
       "Logistic Regression (L2)   [10, 0.07249670803438732]   \n",
       "Linear SVM (L1)            [10, -1.3446957977410057]   \n",
       "Linear SVM (L2)           [10, -0.07445835664585077]   \n",
       "NonLinear SVM (RBF)                            [nan]   \n",
       "Decision Tree             (por, 0.25671081977659505)   \n",
       "Random Forest             (por, 0.25671081977659505)   \n",
       "XGBoost                   (por, 0.25671081977659505)   \n",
       "\n",
       "                                                    2  \n",
       "KNN                                             [nan]  \n",
       "Logistic Regression (L1)   [100, -0.6794881387204452]  \n",
       "Logistic Regression (L2)    [100, 0.1105365630480242]  \n",
       "Linear SVM (L1)            [100, 0.46191392052260766]  \n",
       "Linear SVM (L2)           [100, -0.06556584699866412]  \n",
       "NonLinear SVM (RBF)                             [nan]  \n",
       "Decision Tree             (covid, 0.5129546399705798)  \n",
       "Random Forest             (covid, 0.5129546399705798)  \n",
       "XGBoost                   (covid, 0.5129546399705798)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "LogisticRegression(max_iter=1000, n_jobs=-1, penalty='l1', solver='liblinear')\n",
      "LogisticRegression(max_iter=1000, n_jobs=-1)\n",
      "LinearSVC(dual=False, max_iter=10000, penalty='l1')\n",
      "LinearSVC(max_iter=10000)\n",
      "SVC()\n",
      "DecisionTreeClassifier(max_depth=range(1, 15), random_state=0)\n",
      "RandomForestClassifier(max_depth=range(1, 15), random_state=0)\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=range(1, 15),\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=0, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "knn_c = KNeighborsClassifier(n_jobs=-1)\n",
    "log_res_l1 = LogisticRegression(penalty='l1', max_iter=1000, solver='liblinear', n_jobs=-1)\n",
    "log_res_l2 = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "lin_svc_l1 = LinearSVC(penalty='l1', dual=False, max_iter=10000)\n",
    "lin_svc_l2 = LinearSVC(penalty='l2', max_iter=10000)\n",
    "# cl7 = SVC(kernel='poly', degree=3)\n",
    "depth_settings = range(1, 15)\n",
    "nsvm_rbf = SVC(kernel='rbf')\n",
    "d_tree = DecisionTreeClassifier(random_state=0, max_depth=depth_settings)\n",
    "r_forest = RandomForestClassifier(max_depth=depth_settings, random_state=0)\n",
    "xg_boost = XGBClassifier.XGBClassifier(random_state=0, max_depth=depth_settings, learning_rate=0.01)\n",
    "\n",
    "model_list = [knn_c, log_res_l1, log_res_l2, lin_svc_l1, lin_svc_l2, nsvm_rbf, d_tree, r_forest, xg_boost]\n",
    "model_dict = {}\n",
    "\n",
    "f1_scores_micro = []\n",
    "f1_scores_macro = []\n",
    "f1_scores_weighted = []\n",
    "\n",
    "precision_scores_micro = []\n",
    "precision_scores_macro = []\n",
    "precision_scores_weighted = []\n",
    "\n",
    "recall_scores_micro = []\n",
    "recall_scores_macro = []\n",
    "recall_scores_weighted = []\n",
    "\n",
    "counter = 0\n",
    "model_predict_df = pd.DataFrame()\n",
    "for index, row in model_performances.iterrows():\n",
    "    model_dict[index] = model_list[counter]\n",
    "    curr_model = model_list[counter]\n",
    "    print(curr_model)\n",
    "    curr_model.set_params(**row['Best Parameters'])\n",
    "    trained_curr_model = curr_model.fit(X_train, y_train)\n",
    "    predictions = trained_curr_model.predict(X_val)\n",
    "    \n",
    "    f1_scores_micro.append(f1_score(predictions, y_val, average='micro'))\n",
    "    f1_scores_macro.append(f1_score(predictions, y_val, average='macro'))\n",
    "    f1_scores_weighted.append(f1_score(predictions, y_val, average='weighted'))\n",
    "    \n",
    "    precision_scores_micro.append(precision_score(predictions, y_val, average='micro'))\n",
    "    precision_scores_macro.append(precision_score(predictions, y_val, average='macro'))\n",
    "    precision_scores_weighted.append(precision_score(predictions, y_val, average='weighted'))\n",
    "    \n",
    "    recall_scores_micro.append(recall_score(predictions, y_val, average='micro'))\n",
    "    recall_scores_macro.append(recall_score(predictions, y_val, average='macro'))\n",
    "    recall_scores_weighted.append(recall_score(predictions, y_val, average='weighted'))\n",
    "    \n",
    "    model_predict_df[index] = predictions\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performances['F1 Score Micro'] = f1_scores_micro\n",
    "model_performances['F1 Score Macro'] = f1_scores_macro\n",
    "model_performances['F1 Score Weighted'] = f1_scores_weighted\n",
    "\n",
    "model_performances['Recall Score Micro'] = recall_scores_micro\n",
    "model_performances['Recall Score Macro'] = recall_scores_macro\n",
    "model_performances['Recall Score Weighted'] = recall_scores_weighted\n",
    "\n",
    "model_performances['Precision Score Micro'] = precision_scores_micro\n",
    "model_performances['Precision Score Macro'] = precision_scores_macro\n",
    "model_performances['Precision Score Weighted'] = precision_scores_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Test Accuracy / PCC</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>F1 Score Micro</th>\n",
       "      <th>F1 Score Macro</th>\n",
       "      <th>F1 Score Weighted</th>\n",
       "      <th>Recall Score Micro</th>\n",
       "      <th>Recall Score Macro</th>\n",
       "      <th>Recall Score Weighted</th>\n",
       "      <th>Precision Score Micro</th>\n",
       "      <th>Precision Score Macro</th>\n",
       "      <th>Precision Score Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>4.638301</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.328979</td>\n",
       "      <td>0.649119</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.382456</td>\n",
       "      <td>0.886771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L1)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>0.806248</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.3943897028169658]</td>\n",
       "      <td>[10, -0.710828787650146]</td>\n",
       "      <td>[100, -0.6794881387204452]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.515058</td>\n",
       "      <td>0.670149</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.749104</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.499123</td>\n",
       "      <td>0.838336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L2)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.021544346900318846}</td>\n",
       "      <td>2.171675</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.0898000780119623]</td>\n",
       "      <td>[10, 0.07249670803438732]</td>\n",
       "      <td>[100, 0.1105365630480242]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.671197</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.499123</td>\n",
       "      <td>0.839687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L1)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 3.593813663804626}</td>\n",
       "      <td>3.229808</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, 0.9335408437389131]</td>\n",
       "      <td>[10, -1.3446957977410057]</td>\n",
       "      <td>[100, 0.46191392052260766]</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.670528</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.667391</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.613158</td>\n",
       "      <td>0.725889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L2)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.0016681005372000592}</td>\n",
       "      <td>0.868690</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, -0.03815140571885928]</td>\n",
       "      <td>[10, -0.07445835664585077]</td>\n",
       "      <td>[100, -0.06556584699866412]</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.690541</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.762452</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.532456</td>\n",
       "      <td>0.820910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonLinear SVM (RBF)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>{'C': 599.4842503189421, 'gamma': 1e-05}</td>\n",
       "      <td>11.626434</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.671197</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.499123</td>\n",
       "      <td>0.839687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>1.087498</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>(lgus, 0.2303345402528252)</td>\n",
       "      <td>(por, 0.25671081977659505)</td>\n",
       "      <td>(covid, 0.5129546399705798)</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.627252</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.320402</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.398246</td>\n",
       "      <td>0.766145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.751724</td>\n",
       "      <td>{'max_depth': 14}</td>\n",
       "      <td>7.377641</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>(lgus, 0.2303345402528252)</td>\n",
       "      <td>(por, 0.25671081977659505)</td>\n",
       "      <td>(covid, 0.5129546399705798)</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.328979</td>\n",
       "      <td>0.649119</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.382456</td>\n",
       "      <td>0.886771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.737931</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>65.948250</td>\n",
       "      <td>1.773180</td>\n",
       "      <td>(lgus, 0.2303345402528252)</td>\n",
       "      <td>(por, 0.25671081977659505)</td>\n",
       "      <td>(covid, 0.5129546399705798)</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.710089</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.582456</td>\n",
       "      <td>0.804836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Test Accuracy  Training Accuracy  \\\n",
       "KNN                            0.540541           0.724138   \n",
       "Logistic Regression (L1)       0.621622           0.862069   \n",
       "Logistic Regression (L2)       0.621622           0.972414   \n",
       "Linear SVM (L1)                0.648649           1.000000   \n",
       "Linear SVM (L2)                0.648649           0.972414   \n",
       "NonLinear SVM (RBF)            0.621622           0.979310   \n",
       "Decision Tree                  0.540541           0.613793   \n",
       "Random Forest                  0.540541           0.751724   \n",
       "XGBoost                        0.675676           0.737931   \n",
       "\n",
       "                                                   Best Parameters   Run Time  \\\n",
       "KNN                                            {'n_neighbors': 12}   4.638301   \n",
       "Logistic Regression (L1)                 {'C': 0.2782559402207126}   0.806248   \n",
       "Logistic Regression (L2)               {'C': 0.021544346900318846}   2.171675   \n",
       "Linear SVM (L1)                           {'C': 3.593813663804626}   3.229808   \n",
       "Linear SVM (L2)                       {'C': 0.0016681005372000592}   0.868690   \n",
       "NonLinear SVM (RBF)       {'C': 599.4842503189421, 'gamma': 1e-05}  11.626434   \n",
       "Decision Tree                                     {'max_depth': 2}   1.087498   \n",
       "Random Forest                                    {'max_depth': 14}   7.377641   \n",
       "XGBoost                                           {'max_depth': 5}  65.948250   \n",
       "\n",
       "                          Test Accuracy / PCC                            0  \\\n",
       "KNN                                  1.418544                        [nan]   \n",
       "Logistic Regression (L1)             1.631326    [000, 0.3943897028169658]   \n",
       "Logistic Regression (L2)             1.631326    [000, 0.0898000780119623]   \n",
       "Linear SVM (L1)                      1.702253    [000, 0.9335408437389131]   \n",
       "Linear SVM (L2)                      1.702253  [000, -0.03815140571885928]   \n",
       "NonLinear SVM (RBF)                  1.631326                        [nan]   \n",
       "Decision Tree                        1.418544   (lgus, 0.2303345402528252)   \n",
       "Random Forest                        1.418544   (lgus, 0.2303345402528252)   \n",
       "XGBoost                              1.773180   (lgus, 0.2303345402528252)   \n",
       "\n",
       "                                                   1  \\\n",
       "KNN                                            [nan]   \n",
       "Logistic Regression (L1)    [10, -0.710828787650146]   \n",
       "Logistic Regression (L2)   [10, 0.07249670803438732]   \n",
       "Linear SVM (L1)            [10, -1.3446957977410057]   \n",
       "Linear SVM (L2)           [10, -0.07445835664585077]   \n",
       "NonLinear SVM (RBF)                            [nan]   \n",
       "Decision Tree             (por, 0.25671081977659505)   \n",
       "Random Forest             (por, 0.25671081977659505)   \n",
       "XGBoost                   (por, 0.25671081977659505)   \n",
       "\n",
       "                                                    2  F1 Score Micro  \\\n",
       "KNN                                             [nan]        0.540541   \n",
       "Logistic Regression (L1)   [100, -0.6794881387204452]        0.621622   \n",
       "Logistic Regression (L2)    [100, 0.1105365630480242]        0.621622   \n",
       "Linear SVM (L1)            [100, 0.46191392052260766]        0.648649   \n",
       "Linear SVM (L2)           [100, -0.06556584699866412]        0.648649   \n",
       "NonLinear SVM (RBF)                             [nan]        0.621622   \n",
       "Decision Tree             (covid, 0.5129546399705798)        0.540541   \n",
       "Random Forest             (covid, 0.5129546399705798)        0.540541   \n",
       "XGBoost                   (covid, 0.5129546399705798)        0.675676   \n",
       "\n",
       "                          F1 Score Macro  F1 Score Weighted  \\\n",
       "KNN                             0.328979           0.649119   \n",
       "Logistic Regression (L1)        0.515058           0.670149   \n",
       "Logistic Regression (L2)        0.516190           0.671197   \n",
       "Linear SVM (L1)                 0.603175           0.670528   \n",
       "Linear SVM (L2)                 0.550000           0.690541   \n",
       "NonLinear SVM (RBF)             0.516190           0.671197   \n",
       "Decision Tree                   0.347222           0.627252   \n",
       "Random Forest                   0.328979           0.649119   \n",
       "XGBoost                         0.594203           0.710089   \n",
       "\n",
       "                          Recall Score Micro  Recall Score Macro  \\\n",
       "KNN                                 0.540541            0.398693   \n",
       "Logistic Regression (L1)            0.621622            0.749104   \n",
       "Logistic Regression (L2)            0.621622            0.776882   \n",
       "Linear SVM (L1)                     0.648649            0.667391   \n",
       "Linear SVM (L2)                     0.648649            0.762452   \n",
       "NonLinear SVM (RBF)                 0.621622            0.776882   \n",
       "Decision Tree                       0.540541            0.320402   \n",
       "Random Forest                       0.540541            0.398693   \n",
       "XGBoost                             0.675676            0.694444   \n",
       "\n",
       "                          Recall Score Weighted  Precision Score Micro  \\\n",
       "KNN                                    0.540541               0.540541   \n",
       "Logistic Regression (L1)               0.621622               0.621622   \n",
       "Logistic Regression (L2)               0.621622               0.621622   \n",
       "Linear SVM (L1)                        0.648649               0.648649   \n",
       "Linear SVM (L2)                        0.648649               0.648649   \n",
       "NonLinear SVM (RBF)                    0.621622               0.621622   \n",
       "Decision Tree                          0.540541               0.540541   \n",
       "Random Forest                          0.540541               0.540541   \n",
       "XGBoost                                0.675676               0.675676   \n",
       "\n",
       "                          Precision Score Macro  Precision Score Weighted  \n",
       "KNN                                    0.382456                  0.886771  \n",
       "Logistic Regression (L1)               0.499123                  0.838336  \n",
       "Logistic Regression (L2)               0.499123                  0.839687  \n",
       "Linear SVM (L1)                        0.613158                  0.725889  \n",
       "Linear SVM (L2)                        0.532456                  0.820910  \n",
       "NonLinear SVM (RBF)                    0.499123                  0.839687  \n",
       "Decision Tree                          0.398246                  0.766145  \n",
       "Random Forest                          0.382456                  0.886771  \n",
       "XGBoost                                0.582456                  0.804836  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression (L1)</th>\n",
       "      <th>Logistic Regression (L2)</th>\n",
       "      <th>Linear SVM (L1)</th>\n",
       "      <th>Linear SVM (L2)</th>\n",
       "      <th>NonLinear SVM (RBF)</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    KNN  Logistic Regression (L1)  Logistic Regression (L2)  Linear SVM (L1)  \\\n",
       "0    -1                        -1                        -1               -1   \n",
       "1    -1                        -1                        -1               -1   \n",
       "2    -1                        -1                        -1                1   \n",
       "3    -1                        -1                        -1               -1   \n",
       "4    -1                        -1                        -1                0   \n",
       "5    -1                        -1                        -1                0   \n",
       "6    -1                        -1                        -1               -1   \n",
       "7    -1                        -1                        -1                0   \n",
       "8    -1                        -1                        -1                0   \n",
       "9    -1                        -1                        -1               -1   \n",
       "10   -1                        -1                        -1               -1   \n",
       "11    1                         1                         1                1   \n",
       "12   -1                        -1                        -1               -1   \n",
       "13   -1                        -1                        -1               -1   \n",
       "14   -1                         1                         1                1   \n",
       "15   -1                        -1                        -1               -1   \n",
       "16   -1                        -1                        -1               -1   \n",
       "17   -1                        -1                        -1               -1   \n",
       "18   -1                        -1                        -1               -1   \n",
       "19   -1                        -1                        -1               -1   \n",
       "20   -1                        -1                        -1               -1   \n",
       "21   -1                        -1                        -1               -1   \n",
       "22   -1                        -1                        -1               -1   \n",
       "23    1                         0                         1                0   \n",
       "24   -1                        -1                        -1                0   \n",
       "25   -1                         0                         0                0   \n",
       "26   -1                        -1                        -1               -1   \n",
       "27   -1                        -1                        -1               -1   \n",
       "28    1                         1                         1                1   \n",
       "29   -1                         0                         0                0   \n",
       "30   -1                        -1                        -1               -1   \n",
       "31   -1                        -1                        -1                0   \n",
       "32   -1                        -1                        -1               -1   \n",
       "33   -1                        -1                        -1               -1   \n",
       "34   -1                        -1                        -1               -1   \n",
       "35   -1                        -1                        -1               -1   \n",
       "36   -1                        -1                        -1                0   \n",
       "\n",
       "    Linear SVM (L2)  NonLinear SVM (RBF)  Decision Tree  Random Forest  \\\n",
       "0                 1                   -1             -1             -1   \n",
       "1                -1                   -1             -1             -1   \n",
       "2                -1                   -1             -1             -1   \n",
       "3                -1                   -1             -1             -1   \n",
       "4                -1                   -1              1             -1   \n",
       "5                -1                   -1             -1             -1   \n",
       "6                -1                   -1             -1             -1   \n",
       "7                 1                   -1             -1             -1   \n",
       "8                -1                   -1              1             -1   \n",
       "9                -1                   -1             -1             -1   \n",
       "10               -1                   -1             -1             -1   \n",
       "11                1                    1              1              1   \n",
       "12               -1                   -1             -1             -1   \n",
       "13               -1                   -1             -1             -1   \n",
       "14                1                    1             -1             -1   \n",
       "15               -1                   -1             -1             -1   \n",
       "16               -1                   -1             -1             -1   \n",
       "17               -1                   -1             -1             -1   \n",
       "18               -1                   -1             -1             -1   \n",
       "19               -1                   -1             -1             -1   \n",
       "20               -1                   -1             -1             -1   \n",
       "21               -1                   -1             -1             -1   \n",
       "22               -1                   -1             -1             -1   \n",
       "23                1                    1              1              1   \n",
       "24               -1                   -1             -1             -1   \n",
       "25                0                    0              1             -1   \n",
       "26               -1                   -1             -1             -1   \n",
       "27               -1                   -1             -1             -1   \n",
       "28                1                    1              1              1   \n",
       "29                0                    0             -1             -1   \n",
       "30               -1                   -1             -1             -1   \n",
       "31               -1                   -1              1             -1   \n",
       "32               -1                   -1             -1             -1   \n",
       "33               -1                   -1             -1             -1   \n",
       "34               -1                   -1             -1             -1   \n",
       "35               -1                   -1             -1             -1   \n",
       "36               -1                   -1              1             -1   \n",
       "\n",
       "    XGBoost  \n",
       "0        -1  \n",
       "1        -1  \n",
       "2        -1  \n",
       "3        -1  \n",
       "4         0  \n",
       "5        -1  \n",
       "6        -1  \n",
       "7        -1  \n",
       "8        -1  \n",
       "9        -1  \n",
       "10       -1  \n",
       "11        1  \n",
       "12       -1  \n",
       "13       -1  \n",
       "14        1  \n",
       "15       -1  \n",
       "16       -1  \n",
       "17       -1  \n",
       "18       -1  \n",
       "19       -1  \n",
       "20       -1  \n",
       "21       -1  \n",
       "22        0  \n",
       "23        1  \n",
       "24        0  \n",
       "25       -1  \n",
       "26       -1  \n",
       "27       -1  \n",
       "28        1  \n",
       "29        0  \n",
       "30       -1  \n",
       "31        0  \n",
       "32       -1  \n",
       "33       -1  \n",
       "34       -1  \n",
       "35       -1  \n",
       "36        0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict_df.to_csv('all_model_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithms</th>\n",
       "      <th>Algorithms</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>percent of PCC</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>F1 Score Macro</th>\n",
       "      <th>F1 Score Weighted</th>\n",
       "      <th>Recall Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score Micro</th>\n",
       "      <th>Recall Score Macro</th>\n",
       "      <th>Recall Score Weighted</th>\n",
       "      <th>Precision Score Micro</th>\n",
       "      <th>Precision Score Macro</th>\n",
       "      <th>Precision Score Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>{'n_neighbors': 11}</td>\n",
       "      <td>1.309995</td>\n",
       "      <td>1.347617</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.171171</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression (L1)</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 599.4842503189421}</td>\n",
       "      <td>3.868001</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>['000', -1.2262577545315585]</td>\n",
       "      <td>['0a08dwsjty', -2.8805869509602426]</td>\n",
       "      <td>['0wzxb3n4j9', -2.7253698076951407]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569573</td>\n",
       "      <td>0.686256</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.756152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.830974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression (L2)</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>2.160705</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>['000', -0.16885605165841644]</td>\n",
       "      <td>['0a08dwsjty', 0.2817473148696053]</td>\n",
       "      <td>['0wzxb3n4j9', 0.24544967884729332]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.688224</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.815505</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.815505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Linear SVM (L1)</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>1.866419</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>['000', 0.46233130606259887]</td>\n",
       "      <td>['0a08dwsjty', 0.350727918862724]</td>\n",
       "      <td>['0wzxb3n4j9', 0.37312739526826555]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555319</td>\n",
       "      <td>0.687752</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.798009</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.798009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Linear SVM (L2)</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 0.0016681005372000592}</td>\n",
       "      <td>0.543514</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>['000', -0.04141675364049047]</td>\n",
       "      <td>['0a08dwsjty', -0.06455537526105888]</td>\n",
       "      <td>['0wzxb3n4j9', -0.05927501040747649]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552536</td>\n",
       "      <td>0.685958</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.780512</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.780512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithms                Algorithms  Test Accuracy  Training Accuracy  \\\n",
       "0          0                       KNN       0.513514           0.586207   \n",
       "1          1  Logistic Regression (L1)       0.621622           1.000000   \n",
       "2          2  Logistic Regression (L2)       0.648649           1.000000   \n",
       "3          3           Linear SVM (L1)       0.648649           1.000000   \n",
       "4          4           Linear SVM (L2)       0.648649           1.000000   \n",
       "\n",
       "                Best Parameters  Run Time  percent of PCC  \\\n",
       "0           {'n_neighbors': 11}  1.309995        1.347617   \n",
       "1      {'C': 599.4842503189421}  3.868001        1.631326   \n",
       "2     {'C': 0.2782559402207126}  2.160705        1.702253   \n",
       "3     {'C': 0.2782559402207126}  1.866419        1.702253   \n",
       "4  {'C': 0.0016681005372000592}  0.543514        1.702253   \n",
       "\n",
       "                               0                                     1  \\\n",
       "0                          [nan]                                 [nan]   \n",
       "1   ['000', -1.2262577545315585]   ['0a08dwsjty', -2.8805869509602426]   \n",
       "2  ['000', -0.16885605165841644]    ['0a08dwsjty', 0.2817473148696053]   \n",
       "3   ['000', 0.46233130606259887]     ['0a08dwsjty', 0.350727918862724]   \n",
       "4  ['000', -0.04141675364049047]  ['0a08dwsjty', -0.06455537526105888]   \n",
       "\n",
       "                                      2  ...  F1 Score Macro  \\\n",
       "0                                 [nan]  ...        0.226190   \n",
       "1   ['0wzxb3n4j9', -2.7253698076951407]  ...        0.569573   \n",
       "2   ['0wzxb3n4j9', 0.24544967884729332]  ...        0.559524   \n",
       "3   ['0wzxb3n4j9', 0.37312739526826555]  ...        0.555319   \n",
       "4  ['0wzxb3n4j9', -0.05927501040747649]  ...        0.552536   \n",
       "\n",
       "   F1 Score Weighted  Recall Score  Precision Score  Recall Score Micro  \\\n",
       "0           0.678571      0.513514         1.000000            0.513514   \n",
       "1           0.686256      0.621622         0.756152            0.648649   \n",
       "2           0.688224      0.648649         0.815505            0.648649   \n",
       "3           0.687752      0.648649         0.798009            0.648649   \n",
       "4           0.685958      0.648649         0.780512            0.648649   \n",
       "\n",
       "   Recall Score Macro  Recall Score Weighted  Precision Score Micro  \\\n",
       "0            0.171171               0.513514               0.513514   \n",
       "1            0.783333               0.648649               0.648649   \n",
       "2            0.706897               0.648649               0.648649   \n",
       "3            0.664286               0.648649               0.648649   \n",
       "4            0.638889               0.648649               0.648649   \n",
       "\n",
       "   Precision Score Macro  Precision Score Weighted  \n",
       "0               0.333333                  1.000000  \n",
       "1               0.540789                  0.830974  \n",
       "2               0.540789                  0.815505  \n",
       "3               0.540789                  0.798009  \n",
       "4               0.540789                  0.780512  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances = pd.read_csv('model_performances.csv')\n",
    "model_performances_copy = model_performances.copy()\n",
    "model_performances_copy.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)\n",
    "model_performances_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performances_copy.to_csv('model_performances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Numbers = [3,6,27]\n",
    "\n",
    "my_labels = 'Positive','Neutral','Negative'\n",
    "colors = [\"royalblue\", \"bisque\", \"firebrick\"]\n",
    "plt.pie(Numbers,labels=my_labels,autopct='%1.1f%%', colors=colors)\n",
    "plt.title('XGBoost')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
