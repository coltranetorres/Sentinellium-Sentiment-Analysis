{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:41:50.598331Z",
     "start_time": "2021-01-18T14:41:14.126560Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords.words('english')\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:42:14.578723Z",
     "start_time": "2021-01-18T14:42:13.218695Z"
    }
   },
   "outputs": [],
   "source": [
    "#!dir ..\\..\\1. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:43:29.249501Z",
     "start_time": "2021-01-18T14:43:29.244515Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_tweets = pd.read_csv(r\"../../1. Datasets/NLP Datasets/2. Labelled Tweets/ncr.csv\")\n",
    "df_tweets = pd.read_csv(\"ncr.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:47:40.860660Z",
     "start_time": "2021-01-18T14:47:40.850687Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text, tagalog_letters):\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "    text = tagalog_letters(text)\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = sub(r\"[^A-Za-z0-9^,!?.\\/'+]\", \" \", text)\n",
    "    text = sub(r\"http\\S.*$\",\" \", text) # removing twitter urls\n",
    "    text = sub(r\"\\+\", \" plus \", text)\n",
    "    text = sub(r\",\", \" \", text)\n",
    "    text = sub(r\"\\.\", \" \", text)\n",
    "    text = sub(r\"!\", \" ! \", text)\n",
    "    text = sub(r\"\\?\", \" ? \", text)\n",
    "    text = sub(r\"'\", \"\", text)\n",
    "    text = sub(r\":\", \" : \", text)\n",
    "    text = sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "        \n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        \n",
    "    text = [word for word in text.split(' ') if word not in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:47:46.493406Z",
     "start_time": "2021-01-18T14:47:46.211162Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tweets.tweets = df_tweets.tweets.apply(lambda x: text_to_word_list(x, unidecode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      mayor jerry trenas representing iloilo city go...\n",
       "1      online muna tayo mga kap para sa bayan sa mga ...\n",
       "2      best feeling 8 hours shift covid unit removing...\n",
       "3      dont forget covid 19 safety protocols us still...\n",
       "4      taena nitong covid parang produkto daming vari...\n",
       "                             ...                        \n",
       "177    nakaka sad doctors nurses died frontliners cov...\n",
       "178    dati everyday kung mag update ang doh covid re...\n",
       "179    78 efficacy 100 prevention severe cases covid ...\n",
       "180     kitsunemaisonn chefadobo hahahahaahaa love ko...\n",
       "181    eto kabado na naman kami sa office 3 na pala c...\n",
       "Name: tweets, Length: 182, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Usually splitting is done before vectorization (i.e. the vectorizer shouldn't be fitted with the test set of tweets). However, since we have a small dataset, it is more likely for words in test set to not be available in the train set (which will cause inaccuracies in the model). So in this case, let's just vectorize the full dataset before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:49:13.629390Z",
     "start_time": "2021-01-18T14:49:12.298642Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:50:59.484905Z",
     "start_time": "2021-01-18T14:50:59.468948Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(norm=None)\n",
    "tfidf.fit(df_tweets.tweets)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "X = pd.DataFrame(tfidf.transform(df_tweets.tweets).todense(), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:53:45.915412Z",
     "start_time": "2021-01-18T14:53:45.912420Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df_tweets.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:19.331779Z",
     "start_time": "2021-01-18T14:54:19.305849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (145, 1175)\n",
      "X_val shape:  (37, 1175)\n",
      "y_train shape:  (145,)\n",
      "y_val shape:  (37,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "# X_train_text = X_train['tweets']\n",
    "# X_val_text = X_val['tweets']\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_val shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:34.986268Z",
     "start_time": "2021-01-18T14:54:34.652923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Classifier Modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "stratk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:38.641243Z",
     "start_time": "2021-01-18T14:54:38.253975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6        0.46666667 0.53333333 0.46666667 0.53333333 0.57142857\n",
      " 0.64285714 0.35714286 0.35714286 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "kfold_score = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(kfold_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:54:45.580750Z",
     "start_time": "2021-01-18T14:54:45.575763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kfold logistic regression score\n",
    "round(np.mean(kfold_score)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grids = {'learning_rate': [.2, 0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [3, 4, 6, 10, 14],\n",
    "              'min_samples_leaf': [2, 3, 4],\n",
    "              'max_features': [.5,.3, .2] \n",
    "}   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This code is written a few years ago in my ML class (a bit outdated). Will try to find the updated code and update. This does manual gridsearch of hyperparameters. Actual implementation of GridSearchCV will be faster and better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T15:17:29.385385Z",
     "start_time": "2021-01-18T15:17:29.321544Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import pandas as pd\n",
    "import pylab as plot\n",
    "import lmfit as lf\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pandas import Series\n",
    "from math import sqrt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC, LinearSVC, LinearSVR\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "\n",
    "import numpy\n",
    "\n",
    "def classification(X, y, scaler=None):\n",
    "    kneighbors = range(1, 20)\n",
    "    smoothing = np.logspace(1, 15, num=10)\n",
    "    a = np.logspace(-5, 5, num=10)\n",
    "    gamma_list = np.logspace(-5, 5, num=10)\n",
    "    depth_settings = range(1, 15)\n",
    "    tolerance = np.logspace(-5, 5, num=10)\n",
    "    \n",
    "    knn_c = KNeighborsClassifier(n_jobs=-1)\n",
    "    log_res_l1 = LogisticRegression(penalty='l1', max_iter=1000,\n",
    "                             solver='liblinear', n_jobs=-1)\n",
    "    log_res_l2 = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "    lin_svc_l1 = LinearSVC(penalty='l1', dual=False, max_iter=10000)\n",
    "    lin_svc_l2 = LinearSVC(penalty='l2', max_iter=10000)\n",
    "    # cl7 = SVC(kernel='poly', degree=3)\n",
    "    nsvm_rbf = SVC(kernel='rbf')\n",
    "    d_tree = DecisionTreeRegressor(random_state=0, max_depth=depth_settings)\n",
    "    r_forest = RandomForestRegressor(max_depth=depth_settings, random_state=0)\n",
    "    g_boost = GradientBoostingRegressor(random_state=0, max_depth=depth_settings, learning_rate=0.1)\n",
    "#     mult_nb = MultinomialNB(alpha=a)\n",
    "#     n_bayes = GaussianNB()\n",
    "    \n",
    "\n",
    "    est = [('KNN', knn_c, {'n_neighbors':kneighbors}),\n",
    "           ('Logistic Regression (L1)', log_res_l1, {'C':a}),\n",
    "           ('Logistic Regression (L2)', log_res_l2, {'C':a}),\n",
    "           ('Linear SVM (L1)', lin_svc_l1, {'C':a}),\n",
    "           ('Linear SVM (L2)', lin_svc_l2, {'C':a}),\n",
    "           ('NonLinear SVM (RBF)', nsvm_rbf, {'C':a, 'gamma':gamma_list}),\n",
    "           ('Decision Tree',d_tree,{'max_depth':depth_settings}),\n",
    "           ('Random Forest',r_forest,{'max_depth':depth_settings}),\n",
    "           ('Gradient Boost',g_boost,{'max_depth':depth_settings})]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "    \n",
    "    if scaler == 'ss':\n",
    "        scale = StandardScaler()\n",
    "        X_train = scale.fit_transform(X = X_train)\n",
    "        X_val = scale.transform(X = X_val)\n",
    "    elif scaler == 'mm':\n",
    "        scale = MinMaxScaler()\n",
    "        X_train = scale.fit_transform(X_train)\n",
    "        X_val = scale.transform(X_val)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    models = {}\n",
    "    for e in est:\n",
    "        print(f'Training {e[0]}')\n",
    "        start_time = time.time()\n",
    "        gs_cv = GridSearchCV(e[1], param_grid=e[2], n_jobs=-1)\n",
    "        gs_cv.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        models[e[0]] = gs_cv, elapsed_time\n",
    "        print(f'Training {e[0]} complete in {elapsed_time} seconds \\n')\n",
    "    \n",
    "    accuracy_summary ={}\n",
    "    for m in models:\n",
    "        test_acc = models[m][0].best_estimator_.score(X_val, y_val)\n",
    "        train_acc = models[m][0].best_estimator_.score(X_train, y_train)\n",
    "        best_param = models[m][0].best_params_\n",
    "        accuracy_summary[m] = test_acc, train_acc, best_param, models[m][1]\n",
    "    \n",
    "    target_count = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((target_count/target_count.sum())**2)\n",
    "        \n",
    "    results_summary = pd.DataFrame.from_dict(accuracy_summary, orient='index', columns=['Test Accuracy', 'Training Accuracy', 'Best Parameters', 'Run Time'])\n",
    "    results_summary['Test Accuracy / PCC'] = results_summary['Test Accuracy']/pcc\n",
    "    \n",
    "    #linear\n",
    "    summary={}\n",
    "    methods_l = ['Logistic Regression (L1)','Logistic Regression (L2)',\n",
    "              'Linear SVM (L1)','Linear SVM (L2)']\n",
    "\n",
    "    for m in methods_l:\n",
    "        pred = []\n",
    "        for i,j in list(zip(X.columns, (models[m][0].best_estimator_.coef_))):\n",
    "            pred.append([i,j[np.abs(j).argmax()]])\n",
    "            summary[m] = pred\n",
    "\n",
    "    #ensamble\n",
    "    methods_e = ['Decision Tree', 'Random Forest','Gradient Boost']\n",
    "    pred = []\n",
    "    for m in methods_e:\n",
    "        coefs = [(X.columns[(models[m][0].best_estimator_.feature_importances_).argsort()[-3:]\n",
    "                           ], models[m][0].best_estimator_.feature_importances_[\n",
    "            (models[m][0].best_estimator_.feature_importances_).argsort()[-3:]])]\n",
    "\n",
    "        coeff = list(zip(*coefs[0]))\n",
    "        pred.append(coeff)\n",
    "        summary[m] = pred[0]\n",
    "\n",
    "    #no coeff\n",
    "    methods_no = ['KNN','NonLinear SVM (RBF)']\n",
    "    for m in methods_no:\n",
    "        summary[m] = [[np.nan],[np.nan],[np.nan]]\n",
    "\n",
    "    df_summary_c1 = pd.DataFrame.from_dict(summary)\n",
    "        \n",
    "    return results_summary.join(df_summary_c1.T, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T15:18:52.979704Z",
     "start_time": "2021-01-18T15:17:30.027287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN\n",
      "Training KNN complete in 7.321654319763184 seconds \n",
      "\n",
      "Training Logistic Regression (L1)\n",
      "Training Logistic Regression (L1) complete in 4.837061405181885 seconds \n",
      "\n",
      "Training Logistic Regression (L2)\n",
      "Training Logistic Regression (L2) complete in 2.7555737495422363 seconds \n",
      "\n",
      "Training Linear SVM (L1)\n",
      "Training Linear SVM (L1) complete in 3.331751823425293 seconds \n",
      "\n",
      "Training Linear SVM (L2)\n",
      "Training Linear SVM (L2) complete in 0.8519821166992188 seconds \n",
      "\n",
      "Training NonLinear SVM (RBF)\n",
      "Training NonLinear SVM (RBF) complete in 12.799074649810791 seconds \n",
      "\n",
      "Training Decision Tree\n",
      "Training Decision Tree complete in 1.3939199447631836 seconds \n",
      "\n",
      "Training Random Forest\n",
      "Training Random Forest complete in 14.756024837493896 seconds \n",
      "\n",
      "Training Gradient Boost\n",
      "Training Gradient Boost complete in 21.598822832107544 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performances = classification(X,y,scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Test Accuracy / PCC</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>7.321654</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L1)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>4.837061</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.39437235844766866]</td>\n",
       "      <td>[10, -0.7119148152737039]</td>\n",
       "      <td>[100, -0.6805927841233497]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L2)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.021544346900318846}</td>\n",
       "      <td>2.755574</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.0898000780119623]</td>\n",
       "      <td>[10, 0.07249670803438732]</td>\n",
       "      <td>[100, 0.1105365630480242]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L1)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 3.593813663804626}</td>\n",
       "      <td>3.331752</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, 0.9333725792831185]</td>\n",
       "      <td>[10, -1.3447509005327059]</td>\n",
       "      <td>[100, 0.46186290307403494]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L2)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.0016681005372000592}</td>\n",
       "      <td>0.851982</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, -0.03815139313692919]</td>\n",
       "      <td>[10, -0.07445834617796691]</td>\n",
       "      <td>[100, -0.06556592373105262]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonLinear SVM (RBF)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>{'C': 599.4842503189421, 'gamma': 1e-05}</td>\n",
       "      <td>12.799075</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.043559</td>\n",
       "      <td>0.108689</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>1.393920</td>\n",
       "      <td>0.114313</td>\n",
       "      <td>(handa, 0.0)</td>\n",
       "      <td>(helpers, 0.0)</td>\n",
       "      <td>(covid, 1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.076704</td>\n",
       "      <td>0.137877</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>14.756025</td>\n",
       "      <td>0.201294</td>\n",
       "      <td>(handa, 0.0)</td>\n",
       "      <td>(helpers, 0.0)</td>\n",
       "      <td>(covid, 1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost</th>\n",
       "      <td>0.208289</td>\n",
       "      <td>0.682044</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>21.598823</td>\n",
       "      <td>0.546613</td>\n",
       "      <td>(handa, 0.0)</td>\n",
       "      <td>(helpers, 0.0)</td>\n",
       "      <td>(covid, 1.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Test Accuracy  Training Accuracy  \\\n",
       "KNN                            0.540541           0.724138   \n",
       "Logistic Regression (L1)       0.621622           0.862069   \n",
       "Logistic Regression (L2)       0.621622           0.972414   \n",
       "Linear SVM (L1)                0.648649           1.000000   \n",
       "Linear SVM (L2)                0.648649           0.972414   \n",
       "NonLinear SVM (RBF)            0.621622           0.979310   \n",
       "Decision Tree                  0.043559           0.108689   \n",
       "Random Forest                  0.076704           0.137877   \n",
       "Gradient Boost                 0.208289           0.682044   \n",
       "\n",
       "                                                   Best Parameters   Run Time  \\\n",
       "KNN                                            {'n_neighbors': 12}   7.321654   \n",
       "Logistic Regression (L1)                 {'C': 0.2782559402207126}   4.837061   \n",
       "Logistic Regression (L2)               {'C': 0.021544346900318846}   2.755574   \n",
       "Linear SVM (L1)                           {'C': 3.593813663804626}   3.331752   \n",
       "Linear SVM (L2)                       {'C': 0.0016681005372000592}   0.851982   \n",
       "NonLinear SVM (RBF)       {'C': 599.4842503189421, 'gamma': 1e-05}  12.799075   \n",
       "Decision Tree                                     {'max_depth': 1}   1.393920   \n",
       "Random Forest                                     {'max_depth': 1}  14.756025   \n",
       "Gradient Boost                                    {'max_depth': 2}  21.598823   \n",
       "\n",
       "                          Test Accuracy / PCC                            0  \\\n",
       "KNN                                  1.418544                        [nan]   \n",
       "Logistic Regression (L1)             1.631326   [000, 0.39437235844766866]   \n",
       "Logistic Regression (L2)             1.631326    [000, 0.0898000780119623]   \n",
       "Linear SVM (L1)                      1.702253    [000, 0.9333725792831185]   \n",
       "Linear SVM (L2)                      1.702253  [000, -0.03815139313692919]   \n",
       "NonLinear SVM (RBF)                  1.631326                        [nan]   \n",
       "Decision Tree                        0.114313                 (handa, 0.0)   \n",
       "Random Forest                        0.201294                 (handa, 0.0)   \n",
       "Gradient Boost                       0.546613                 (handa, 0.0)   \n",
       "\n",
       "                                                   1  \\\n",
       "KNN                                            [nan]   \n",
       "Logistic Regression (L1)   [10, -0.7119148152737039]   \n",
       "Logistic Regression (L2)   [10, 0.07249670803438732]   \n",
       "Linear SVM (L1)            [10, -1.3447509005327059]   \n",
       "Linear SVM (L2)           [10, -0.07445834617796691]   \n",
       "NonLinear SVM (RBF)                            [nan]   \n",
       "Decision Tree                         (helpers, 0.0)   \n",
       "Random Forest                         (helpers, 0.0)   \n",
       "Gradient Boost                        (helpers, 0.0)   \n",
       "\n",
       "                                                    2  \n",
       "KNN                                             [nan]  \n",
       "Logistic Regression (L1)   [100, -0.6805927841233497]  \n",
       "Logistic Regression (L2)    [100, 0.1105365630480242]  \n",
       "Linear SVM (L1)            [100, 0.46186290307403494]  \n",
       "Linear SVM (L2)           [100, -0.06556592373105262]  \n",
       "NonLinear SVM (RBF)                             [nan]  \n",
       "Decision Tree                            (covid, 1.0)  \n",
       "Random Forest                            (covid, 1.0)  \n",
       "Gradient Boost                           (covid, 1.0)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "LogisticRegression(max_iter=1000, n_jobs=-1, penalty='l1', solver='liblinear')\n",
      "LogisticRegression(max_iter=1000, n_jobs=-1)\n",
      "LinearSVC(dual=False, max_iter=10000, penalty='l1')\n",
      "LinearSVC(max_iter=10000)\n",
      "SVC()\n",
      "DecisionTreeClassifier(max_depth=range(1, 15), random_state=0)\n",
      "RandomForestClassifier(max_depth=range(1, 15), random_state=0)\n",
      "GradientBoostingClassifier(max_depth=range(1, 15), random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "knn_c = KNeighborsClassifier(n_jobs=-1)\n",
    "log_res_l1 = LogisticRegression(penalty='l1', max_iter=1000, solver='liblinear', n_jobs=-1)\n",
    "log_res_l2 = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "lin_svc_l1 = LinearSVC(penalty='l1', dual=False, max_iter=10000)\n",
    "lin_svc_l2 = LinearSVC(penalty='l2', max_iter=10000)\n",
    "# cl7 = SVC(kernel='poly', degree=3)\n",
    "depth_settings = range(1, 15)\n",
    "nsvm_rbf = SVC(kernel='rbf')\n",
    "d_tree = DecisionTreeClassifier(random_state=0, max_depth=depth_settings)\n",
    "r_forest = RandomForestClassifier(max_depth=depth_settings, random_state=0)\n",
    "g_boost = GradientBoostingClassifier(random_state=0, max_depth=depth_settings, learning_rate=0.1)\n",
    "\n",
    "model_list = [knn_c, log_res_l1, log_res_l2, lin_svc_l1, lin_svc_l2, nsvm_rbf, d_tree, r_forest, g_boost]\n",
    "model_dict = {}\n",
    "\n",
    "f1_scores_micro = []\n",
    "f1_scores_macro = []\n",
    "f1_scores_weighted = []\n",
    "\n",
    "precision_scores_micro = []\n",
    "precision_scores_macro = []\n",
    "precision_scores_weighted = []\n",
    "\n",
    "recall_scores_micro = []\n",
    "recall_scores_macro = []\n",
    "recall_scores_weighted = []\n",
    "\n",
    "counter = 0\n",
    "model_predict_df = pd.DataFrame()\n",
    "for index, row in model_performances.iterrows():\n",
    "    model_dict[index] = model_list[counter]\n",
    "    curr_model = model_list[counter]\n",
    "    print(curr_model)\n",
    "    curr_model.set_params(**row['Best Parameters'])\n",
    "    trained_curr_model = curr_model.fit(X_train, y_train)\n",
    "    predictions = trained_curr_model.predict(X_val)\n",
    "    \n",
    "    f1_scores_micro.append(f1_score(predictions, y_val, average='micro'))\n",
    "    f1_scores_macro.append(f1_score(predictions, y_val, average='macro'))\n",
    "    f1_scores_weighted.append(f1_score(predictions, y_val, average='weighted'))\n",
    "    \n",
    "    precision_scores_micro.append(precision_score(predictions, y_val, average='micro'))\n",
    "    precision_scores_macro.append(precision_score(predictions, y_val, average='macro'))\n",
    "    precision_scores_weighted.append(precision_score(predictions, y_val, average='weighted'))\n",
    "    \n",
    "    recall_scores_micro.append(recall_score(predictions, y_val, average='micro'))\n",
    "    recall_scores_macro.append(recall_score(predictions, y_val, average='macro'))\n",
    "    recall_scores_weighted.append(recall_score(predictions, y_val, average='weighted'))\n",
    "    \n",
    "    model_predict_df[index] = predictions\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performances['F1 Score Micro'] = f1_scores_micro\n",
    "model_performances['F1 Score Macro'] = f1_scores_macro\n",
    "model_performances['F1 Score Weighted'] = f1_scores_weighted\n",
    "\n",
    "model_performances['Recall Score Micro'] = recall_scores_micro\n",
    "model_performances['Recall Score Macro'] = recall_scores_macro\n",
    "model_performances['Recall Score Weighted'] = recall_scores_weighted\n",
    "\n",
    "model_performances['Precision Score Micro'] = precision_scores_micro\n",
    "model_performances['Precision Score Macro'] = precision_scores_macro\n",
    "model_performances['Precision Score Weighted'] = precision_scores_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Test Accuracy / PCC</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>F1 Score Micro</th>\n",
       "      <th>F1 Score Macro</th>\n",
       "      <th>F1 Score Weighted</th>\n",
       "      <th>Recall Score Micro</th>\n",
       "      <th>Recall Score Macro</th>\n",
       "      <th>Recall Score Weighted</th>\n",
       "      <th>Precision Score Micro</th>\n",
       "      <th>Precision Score Macro</th>\n",
       "      <th>Precision Score Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>7.321654</td>\n",
       "      <td>1.418544</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.328979</td>\n",
       "      <td>0.649119</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.382456</td>\n",
       "      <td>0.886771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L1)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>4.837061</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.39437235844766866]</td>\n",
       "      <td>[10, -0.7119148152737039]</td>\n",
       "      <td>[100, -0.6805927841233497]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.515058</td>\n",
       "      <td>0.670149</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.749104</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.499123</td>\n",
       "      <td>0.838336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (L2)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.021544346900318846}</td>\n",
       "      <td>2.755574</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[000, 0.0898000780119623]</td>\n",
       "      <td>[10, 0.07249670803438732]</td>\n",
       "      <td>[100, 0.1105365630480242]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.671197</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.499123</td>\n",
       "      <td>0.839687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L1)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 3.593813663804626}</td>\n",
       "      <td>3.331752</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, 0.9333725792831185]</td>\n",
       "      <td>[10, -1.3447509005327059]</td>\n",
       "      <td>[100, 0.46186290307403494]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.571494</td>\n",
       "      <td>0.641962</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.643519</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.571491</td>\n",
       "      <td>0.696550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM (L2)</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>{'C': 0.0016681005372000592}</td>\n",
       "      <td>0.851982</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>[000, -0.03815139313692919]</td>\n",
       "      <td>[10, -0.07445834617796691]</td>\n",
       "      <td>[100, -0.06556592373105262]</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.690541</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.762452</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.532456</td>\n",
       "      <td>0.820910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonLinear SVM (RBF)</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>{'C': 599.4842503189421, 'gamma': 1e-05}</td>\n",
       "      <td>12.799075</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.671197</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.499123</td>\n",
       "      <td>0.839687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.043559</td>\n",
       "      <td>0.108689</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>1.393920</td>\n",
       "      <td>0.114313</td>\n",
       "      <td>(handa, 0.0)</td>\n",
       "      <td>(helpers, 0.0)</td>\n",
       "      <td>(covid, 1.0)</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.627252</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.320402</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.398246</td>\n",
       "      <td>0.766145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.076704</td>\n",
       "      <td>0.137877</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>14.756025</td>\n",
       "      <td>0.201294</td>\n",
       "      <td>(handa, 0.0)</td>\n",
       "      <td>(helpers, 0.0)</td>\n",
       "      <td>(covid, 1.0)</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.171171</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost</th>\n",
       "      <td>0.208289</td>\n",
       "      <td>0.682044</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>21.598823</td>\n",
       "      <td>0.546613</td>\n",
       "      <td>(handa, 0.0)</td>\n",
       "      <td>(helpers, 0.0)</td>\n",
       "      <td>(covid, 1.0)</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.599641</td>\n",
       "      <td>0.709730</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.582456</td>\n",
       "      <td>0.822333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Test Accuracy  Training Accuracy  \\\n",
       "KNN                            0.540541           0.724138   \n",
       "Logistic Regression (L1)       0.621622           0.862069   \n",
       "Logistic Regression (L2)       0.621622           0.972414   \n",
       "Linear SVM (L1)                0.648649           1.000000   \n",
       "Linear SVM (L2)                0.648649           0.972414   \n",
       "NonLinear SVM (RBF)            0.621622           0.979310   \n",
       "Decision Tree                  0.043559           0.108689   \n",
       "Random Forest                  0.076704           0.137877   \n",
       "Gradient Boost                 0.208289           0.682044   \n",
       "\n",
       "                                                   Best Parameters   Run Time  \\\n",
       "KNN                                            {'n_neighbors': 12}   7.321654   \n",
       "Logistic Regression (L1)                 {'C': 0.2782559402207126}   4.837061   \n",
       "Logistic Regression (L2)               {'C': 0.021544346900318846}   2.755574   \n",
       "Linear SVM (L1)                           {'C': 3.593813663804626}   3.331752   \n",
       "Linear SVM (L2)                       {'C': 0.0016681005372000592}   0.851982   \n",
       "NonLinear SVM (RBF)       {'C': 599.4842503189421, 'gamma': 1e-05}  12.799075   \n",
       "Decision Tree                                     {'max_depth': 1}   1.393920   \n",
       "Random Forest                                     {'max_depth': 1}  14.756025   \n",
       "Gradient Boost                                    {'max_depth': 2}  21.598823   \n",
       "\n",
       "                          Test Accuracy / PCC                            0  \\\n",
       "KNN                                  1.418544                        [nan]   \n",
       "Logistic Regression (L1)             1.631326   [000, 0.39437235844766866]   \n",
       "Logistic Regression (L2)             1.631326    [000, 0.0898000780119623]   \n",
       "Linear SVM (L1)                      1.702253    [000, 0.9333725792831185]   \n",
       "Linear SVM (L2)                      1.702253  [000, -0.03815139313692919]   \n",
       "NonLinear SVM (RBF)                  1.631326                        [nan]   \n",
       "Decision Tree                        0.114313                 (handa, 0.0)   \n",
       "Random Forest                        0.201294                 (handa, 0.0)   \n",
       "Gradient Boost                       0.546613                 (handa, 0.0)   \n",
       "\n",
       "                                                   1  \\\n",
       "KNN                                            [nan]   \n",
       "Logistic Regression (L1)   [10, -0.7119148152737039]   \n",
       "Logistic Regression (L2)   [10, 0.07249670803438732]   \n",
       "Linear SVM (L1)            [10, -1.3447509005327059]   \n",
       "Linear SVM (L2)           [10, -0.07445834617796691]   \n",
       "NonLinear SVM (RBF)                            [nan]   \n",
       "Decision Tree                         (helpers, 0.0)   \n",
       "Random Forest                         (helpers, 0.0)   \n",
       "Gradient Boost                        (helpers, 0.0)   \n",
       "\n",
       "                                                    2  F1 Score Micro  \\\n",
       "KNN                                             [nan]        0.540541   \n",
       "Logistic Regression (L1)   [100, -0.6805927841233497]        0.621622   \n",
       "Logistic Regression (L2)    [100, 0.1105365630480242]        0.621622   \n",
       "Linear SVM (L1)            [100, 0.46186290307403494]        0.621622   \n",
       "Linear SVM (L2)           [100, -0.06556592373105262]        0.648649   \n",
       "NonLinear SVM (RBF)                             [nan]        0.621622   \n",
       "Decision Tree                            (covid, 1.0)        0.540541   \n",
       "Random Forest                            (covid, 1.0)        0.513514   \n",
       "Gradient Boost                           (covid, 1.0)        0.675676   \n",
       "\n",
       "                          F1 Score Macro  F1 Score Weighted  \\\n",
       "KNN                             0.328979           0.649119   \n",
       "Logistic Regression (L1)        0.515058           0.670149   \n",
       "Logistic Regression (L2)        0.516190           0.671197   \n",
       "Linear SVM (L1)                 0.571494           0.641962   \n",
       "Linear SVM (L2)                 0.550000           0.690541   \n",
       "NonLinear SVM (RBF)             0.516190           0.671197   \n",
       "Decision Tree                   0.347222           0.627252   \n",
       "Random Forest                   0.226190           0.678571   \n",
       "Gradient Boost                  0.599641           0.709730   \n",
       "\n",
       "                          Recall Score Micro  Recall Score Macro  \\\n",
       "KNN                                 0.540541            0.398693   \n",
       "Logistic Regression (L1)            0.621622            0.749104   \n",
       "Logistic Regression (L2)            0.621622            0.776882   \n",
       "Linear SVM (L1)                     0.621622            0.643519   \n",
       "Linear SVM (L2)                     0.648649            0.762452   \n",
       "NonLinear SVM (RBF)                 0.621622            0.776882   \n",
       "Decision Tree                       0.540541            0.320402   \n",
       "Random Forest                       0.513514            0.171171   \n",
       "Gradient Boost                      0.675676            0.769841   \n",
       "\n",
       "                          Recall Score Weighted  Precision Score Micro  \\\n",
       "KNN                                    0.540541               0.540541   \n",
       "Logistic Regression (L1)               0.621622               0.621622   \n",
       "Logistic Regression (L2)               0.621622               0.621622   \n",
       "Linear SVM (L1)                        0.621622               0.621622   \n",
       "Linear SVM (L2)                        0.648649               0.648649   \n",
       "NonLinear SVM (RBF)                    0.621622               0.621622   \n",
       "Decision Tree                          0.540541               0.540541   \n",
       "Random Forest                          0.513514               0.513514   \n",
       "Gradient Boost                         0.675676               0.675676   \n",
       "\n",
       "                          Precision Score Macro  Precision Score Weighted  \n",
       "KNN                                    0.382456                  0.886771  \n",
       "Logistic Regression (L1)               0.499123                  0.838336  \n",
       "Logistic Regression (L2)               0.499123                  0.839687  \n",
       "Linear SVM (L1)                        0.571491                  0.696550  \n",
       "Linear SVM (L2)                        0.532456                  0.820910  \n",
       "NonLinear SVM (RBF)                    0.499123                  0.839687  \n",
       "Decision Tree                          0.398246                  0.766145  \n",
       "Random Forest                          0.333333                  1.000000  \n",
       "Gradient Boost                         0.582456                  0.822333  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression (L1)</th>\n",
       "      <th>Logistic Regression (L2)</th>\n",
       "      <th>Linear SVM (L1)</th>\n",
       "      <th>Linear SVM (L2)</th>\n",
       "      <th>NonLinear SVM (RBF)</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient Boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    KNN  Logistic Regression (L1)  Logistic Regression (L2)  Linear SVM (L1)  \\\n",
       "0    -1                        -1                        -1               -1   \n",
       "1    -1                        -1                        -1               -1   \n",
       "2    -1                        -1                        -1                1   \n",
       "3    -1                        -1                        -1               -1   \n",
       "4    -1                        -1                        -1                0   \n",
       "5    -1                        -1                        -1                0   \n",
       "6    -1                        -1                        -1               -1   \n",
       "7    -1                        -1                        -1               -1   \n",
       "8    -1                        -1                        -1                0   \n",
       "9    -1                        -1                        -1               -1   \n",
       "10   -1                        -1                        -1               -1   \n",
       "11    1                         1                         1                1   \n",
       "12   -1                        -1                        -1               -1   \n",
       "13   -1                        -1                        -1               -1   \n",
       "14   -1                         1                         1                1   \n",
       "15   -1                        -1                        -1               -1   \n",
       "16   -1                        -1                        -1               -1   \n",
       "17   -1                        -1                        -1               -1   \n",
       "18   -1                        -1                        -1               -1   \n",
       "19   -1                        -1                        -1               -1   \n",
       "20   -1                        -1                        -1               -1   \n",
       "21   -1                        -1                        -1               -1   \n",
       "22   -1                        -1                        -1               -1   \n",
       "23    1                         0                         1                0   \n",
       "24   -1                        -1                        -1                0   \n",
       "25   -1                         0                         0                0   \n",
       "26   -1                        -1                        -1               -1   \n",
       "27   -1                        -1                        -1               -1   \n",
       "28    1                         1                         1                1   \n",
       "29   -1                         0                         0                0   \n",
       "30   -1                        -1                        -1               -1   \n",
       "31   -1                        -1                        -1                0   \n",
       "32   -1                        -1                        -1               -1   \n",
       "33   -1                        -1                        -1               -1   \n",
       "34   -1                        -1                        -1               -1   \n",
       "35   -1                        -1                        -1               -1   \n",
       "36   -1                        -1                        -1                0   \n",
       "\n",
       "    Linear SVM (L2)  NonLinear SVM (RBF)  Decision Tree  Random Forest  \\\n",
       "0                 1                   -1             -1             -1   \n",
       "1                -1                   -1             -1             -1   \n",
       "2                -1                   -1             -1             -1   \n",
       "3                -1                   -1             -1             -1   \n",
       "4                -1                   -1              1             -1   \n",
       "5                -1                   -1             -1             -1   \n",
       "6                -1                   -1             -1             -1   \n",
       "7                 1                   -1             -1             -1   \n",
       "8                -1                   -1              1             -1   \n",
       "9                -1                   -1             -1             -1   \n",
       "10               -1                   -1             -1             -1   \n",
       "11                1                    1              1             -1   \n",
       "12               -1                   -1             -1             -1   \n",
       "13               -1                   -1             -1             -1   \n",
       "14                1                    1             -1             -1   \n",
       "15               -1                   -1             -1             -1   \n",
       "16               -1                   -1             -1             -1   \n",
       "17               -1                   -1             -1             -1   \n",
       "18               -1                   -1             -1             -1   \n",
       "19               -1                   -1             -1             -1   \n",
       "20               -1                   -1             -1             -1   \n",
       "21               -1                   -1             -1             -1   \n",
       "22               -1                   -1             -1             -1   \n",
       "23                1                    1              1             -1   \n",
       "24               -1                   -1             -1             -1   \n",
       "25                0                    0              1             -1   \n",
       "26               -1                   -1             -1             -1   \n",
       "27               -1                   -1             -1             -1   \n",
       "28                1                    1              1             -1   \n",
       "29                0                    0             -1             -1   \n",
       "30               -1                   -1             -1             -1   \n",
       "31               -1                   -1              1             -1   \n",
       "32               -1                   -1             -1             -1   \n",
       "33               -1                   -1             -1             -1   \n",
       "34               -1                   -1             -1             -1   \n",
       "35               -1                   -1             -1             -1   \n",
       "36               -1                   -1              1             -1   \n",
       "\n",
       "    Gradient Boost  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2               -1  \n",
       "3               -1  \n",
       "4                0  \n",
       "5               -1  \n",
       "6               -1  \n",
       "7               -1  \n",
       "8               -1  \n",
       "9               -1  \n",
       "10              -1  \n",
       "11               1  \n",
       "12              -1  \n",
       "13              -1  \n",
       "14               1  \n",
       "15              -1  \n",
       "16              -1  \n",
       "17              -1  \n",
       "18              -1  \n",
       "19              -1  \n",
       "20              -1  \n",
       "21              -1  \n",
       "22              -1  \n",
       "23               0  \n",
       "24               0  \n",
       "25               0  \n",
       "26              -1  \n",
       "27              -1  \n",
       "28               1  \n",
       "29              -1  \n",
       "30              -1  \n",
       "31               0  \n",
       "32              -1  \n",
       "33              -1  \n",
       "34              -1  \n",
       "35              -1  \n",
       "36               0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict_df.to_csv('all_model_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithms</th>\n",
       "      <th>Algorithms</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>percent of PCC</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>F1 Score Macro</th>\n",
       "      <th>F1 Score Weighted</th>\n",
       "      <th>Recall Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score Micro</th>\n",
       "      <th>Recall Score Macro</th>\n",
       "      <th>Recall Score Weighted</th>\n",
       "      <th>Precision Score Micro</th>\n",
       "      <th>Precision Score Macro</th>\n",
       "      <th>Precision Score Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>{'n_neighbors': 11}</td>\n",
       "      <td>1.309995</td>\n",
       "      <td>1.347617</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.171171</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression (L1)</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 599.4842503189421}</td>\n",
       "      <td>3.868001</td>\n",
       "      <td>1.631326</td>\n",
       "      <td>['000', -1.2262577545315585]</td>\n",
       "      <td>['0a08dwsjty', -2.8805869509602426]</td>\n",
       "      <td>['0wzxb3n4j9', -2.7253698076951407]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569573</td>\n",
       "      <td>0.686256</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.756152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.830974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression (L2)</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>2.160705</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>['000', -0.16885605165841644]</td>\n",
       "      <td>['0a08dwsjty', 0.2817473148696053]</td>\n",
       "      <td>['0wzxb3n4j9', 0.24544967884729332]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.688224</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.815505</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.815505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Linear SVM (L1)</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 0.2782559402207126}</td>\n",
       "      <td>1.866419</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>['000', 0.46233130606259887]</td>\n",
       "      <td>['0a08dwsjty', 0.350727918862724]</td>\n",
       "      <td>['0wzxb3n4j9', 0.37312739526826555]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555319</td>\n",
       "      <td>0.687752</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.798009</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.798009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Linear SVM (L2)</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': 0.0016681005372000592}</td>\n",
       "      <td>0.543514</td>\n",
       "      <td>1.702253</td>\n",
       "      <td>['000', -0.04141675364049047]</td>\n",
       "      <td>['0a08dwsjty', -0.06455537526105888]</td>\n",
       "      <td>['0wzxb3n4j9', -0.05927501040747649]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552536</td>\n",
       "      <td>0.685958</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.780512</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.780512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithms                Algorithms  Test Accuracy  Training Accuracy  \\\n",
       "0          0                       KNN       0.513514           0.586207   \n",
       "1          1  Logistic Regression (L1)       0.621622           1.000000   \n",
       "2          2  Logistic Regression (L2)       0.648649           1.000000   \n",
       "3          3           Linear SVM (L1)       0.648649           1.000000   \n",
       "4          4           Linear SVM (L2)       0.648649           1.000000   \n",
       "\n",
       "                Best Parameters  Run Time  percent of PCC  \\\n",
       "0           {'n_neighbors': 11}  1.309995        1.347617   \n",
       "1      {'C': 599.4842503189421}  3.868001        1.631326   \n",
       "2     {'C': 0.2782559402207126}  2.160705        1.702253   \n",
       "3     {'C': 0.2782559402207126}  1.866419        1.702253   \n",
       "4  {'C': 0.0016681005372000592}  0.543514        1.702253   \n",
       "\n",
       "                               0                                     1  \\\n",
       "0                          [nan]                                 [nan]   \n",
       "1   ['000', -1.2262577545315585]   ['0a08dwsjty', -2.8805869509602426]   \n",
       "2  ['000', -0.16885605165841644]    ['0a08dwsjty', 0.2817473148696053]   \n",
       "3   ['000', 0.46233130606259887]     ['0a08dwsjty', 0.350727918862724]   \n",
       "4  ['000', -0.04141675364049047]  ['0a08dwsjty', -0.06455537526105888]   \n",
       "\n",
       "                                      2  ...  F1 Score Macro  \\\n",
       "0                                 [nan]  ...        0.226190   \n",
       "1   ['0wzxb3n4j9', -2.7253698076951407]  ...        0.569573   \n",
       "2   ['0wzxb3n4j9', 0.24544967884729332]  ...        0.559524   \n",
       "3   ['0wzxb3n4j9', 0.37312739526826555]  ...        0.555319   \n",
       "4  ['0wzxb3n4j9', -0.05927501040747649]  ...        0.552536   \n",
       "\n",
       "   F1 Score Weighted  Recall Score  Precision Score  Recall Score Micro  \\\n",
       "0           0.678571      0.513514         1.000000            0.513514   \n",
       "1           0.686256      0.621622         0.756152            0.648649   \n",
       "2           0.688224      0.648649         0.815505            0.648649   \n",
       "3           0.687752      0.648649         0.798009            0.648649   \n",
       "4           0.685958      0.648649         0.780512            0.648649   \n",
       "\n",
       "   Recall Score Macro  Recall Score Weighted  Precision Score Micro  \\\n",
       "0            0.171171               0.513514               0.513514   \n",
       "1            0.783333               0.648649               0.648649   \n",
       "2            0.706897               0.648649               0.648649   \n",
       "3            0.664286               0.648649               0.648649   \n",
       "4            0.638889               0.648649               0.648649   \n",
       "\n",
       "   Precision Score Macro  Precision Score Weighted  \n",
       "0               0.333333                  1.000000  \n",
       "1               0.540789                  0.830974  \n",
       "2               0.540789                  0.815505  \n",
       "3               0.540789                  0.798009  \n",
       "4               0.540789                  0.780512  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances = pd.read_csv('model_performances.csv')\n",
    "model_performances_copy = model_performances.copy()\n",
    "model_performances_copy.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)\n",
    "model_performances_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performances_copy.to_csv('model_performances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Numbers = [4,9,24]\n",
    "\n",
    "my_labels = 'Positive','Neutral','Negative'\n",
    "colors = [\"royalblue\", \"bisque\", \"firebrick\"]\n",
    "plt.pie(Numbers,labels=my_labels,autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Linear SVM (L1)')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
